{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep():\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "def pretty(x):\n",
    "    print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\t2\t1\t4\t0\t3\n",
      "0\t2\t4\t4\t0\t4\n",
      "2\t4\t0\t1\t3\t2\n",
      "3\t1\t0\t2\t1\t1\n",
      "4\t3\t2\t0\t1\t0\n",
      "2\t0\t2\t0\t4\t0\n"
     ]
    }
   ],
   "source": [
    "A = np.random.randint(0, 5, size=(6, 6))\n",
    "pretty(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel:\n",
      "1\t1\t1\n",
      "1\t1\t1\n",
      "1\t1\t1\n"
     ]
    }
   ],
   "source": [
    "kernel = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
    "print(\"Kernel:\")\n",
    "pretty(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(A, kernel):\n",
    "    A_row, A_col = A.shape   # A_row is height, A_col is width\n",
    "    kernel_row, kernel_col = kernel.shape\n",
    "    result = np.zeros((A_row - kernel_row + 1, A_col - kernel_col + 1))\n",
    "\n",
    "    for i in range(result.shape[0]):\n",
    "        for j in range(result.shape[1]):\n",
    "            result[i][j] = np.sum(A[i : i + kernel_row, j : j + kernel_col] * kernel)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(matrix, padding_size):\n",
    "    original_rows, original_cols = matrix.shape\n",
    "\n",
    "    # Create a new matrix filled with zeros, large enough to include the padding\n",
    "    padded_matrix = np.zeros((original_rows + 2 * padding_size, original_cols + 2 * padding_size))\n",
    "    \n",
    "    # Place the original matrix into the center of the padded matrix\n",
    "    padded_matrix[padding_size:-padding_size, padding_size:-padding_size] = matrix\n",
    "    \n",
    "    # Return the padded matrix\n",
    "    return padded_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded A:\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t4.0\t2.0\t1.0\t4.0\t0.0\t3.0\t0.0\n",
      "0.0\t0.0\t2.0\t4.0\t4.0\t0.0\t4.0\t0.0\n",
      "0.0\t2.0\t4.0\t0.0\t1.0\t3.0\t2.0\t0.0\n",
      "0.0\t3.0\t1.0\t0.0\t2.0\t1.0\t1.0\t0.0\n",
      "0.0\t4.0\t3.0\t2.0\t0.0\t1.0\t0.0\t0.0\n",
      "0.0\t2.0\t0.0\t2.0\t0.0\t4.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "[[ 8. 13. 17. 13. 15.  7.]\n",
      " [14. 19. 22. 17. 21. 12.]\n",
      " [12. 16. 18. 15. 18. 11.]\n",
      " [17. 19. 13. 10. 11.  8.]\n",
      " [13. 17. 10. 12.  9.  7.]\n",
      " [ 9. 13.  7.  9.  5.  5.]]\n"
     ]
    }
   ],
   "source": [
    "padded_A = pad(A, 1)\n",
    "print(\"Padded A:\")\n",
    "pretty(padded_A)\n",
    "sep()\n",
    "print(conv2d(padded_A, kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, kernel_size, num_filters):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_filters = num_filters\n",
    "        self.filters = np.random.randn(num_filters, kernel_size, kernel_size) / (kernel_size * kernel_size)\n",
    "\n",
    "    def generate_patches(self, A):\n",
    "        A_row, A_col = A.shape\n",
    "        for i in range(A_row - self.kernel_size + 1):\n",
    "            for j in range(A_col - self.kernel_size + 1):\n",
    "                patch = A[i : i + self.kernel_size, j : j + self.kernel_size]\n",
    "                yield patch, i, j\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.last_input = A\n",
    "        A_row, A_col = A.shape\n",
    "        output = np.zeros((A_row - self.kernel_size + 1, A_col - self.kernel_size + 1, self.num_filters))\n",
    "\n",
    "        for patch, i, j in self.generate_patches(A):\n",
    "            output[i, j] = np.sum(patch * self.filters, axis=(1, 2))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_loss_d_output, learning_rate):\n",
    "        d_loss_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "        # Iterate over patches in the last input \n",
    "        for patch, row, col in self.generate_patches(self.last_input):\n",
    "            # Update the gradient of the filters\n",
    "            for filter_index in range(self.num_filters):\n",
    "                d_loss_d_filters[filter_index] += d_loss_d_output[row, col, filter_index] * patch\n",
    "\n",
    "        # Update filters using the calculated gradient and learning rate\n",
    "        self.filters -= learning_rate * d_loss_d_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3)\n",
      "[[[ 0.05024648 -0.02182262 -0.03723842]\n",
      "  [-0.11217252  0.04902679 -0.15616133]\n",
      "  [ 0.22818769 -0.02798445 -0.11776293]]\n",
      "\n",
      " [[-0.04799268 -0.03247145 -0.07324366]\n",
      "  [ 0.04739954 -0.01275943 -0.15069858]\n",
      "  [-0.09307474 -0.1088065   0.13565627]]]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "(array([[0., 0., 0.],\n",
      "       [0., 4., 2.],\n",
      "       [0., 0., 2.]]), 0, 0)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "(6, 6, 2)\n",
      "[[-0.35174134 -1.03381844 -0.92657768  0.88474763 -0.47547502  0.0351426 ]\n",
      " [-1.00111057 -0.06205186  0.07182133 -0.67090015 -1.07535623  0.75923349]\n",
      " [-0.80278482  0.43574185 -0.74794076 -0.47949357  0.08524546 -0.12555121]\n",
      " [-0.66690642  0.31898328  0.3678465   0.14696684 -0.44916224  0.27213616]\n",
      " [-0.43105198 -0.26416602 -0.31866322 -0.47606636 -0.02147907  0.8290021 ]\n",
      " [-0.10095214 -0.47562647  0.20514779 -0.78573581  0.17428455 -0.39844361]]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_layer = ConvolutionLayer(3, 2)\n",
    "print(cnn_layer.filters.shape)\n",
    "print(cnn_layer.filters)\n",
    "sep()\n",
    "\n",
    "patches = cnn_layer.generate_patches(padded_A)\n",
    "print(next(patches))\n",
    "sep()\n",
    "\n",
    "output = cnn_layer.forward(padded_A)\n",
    "print(output.shape)\n",
    "print(output[:, :, 0])\n",
    "sep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d:\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "    \n",
    "    def generate_patches(self, A):\n",
    "        A_row, A_col, num_filters = A.shape\n",
    "        for i in range(0, A_row, self.pool_size):\n",
    "            for j in range(0, A_col, self.pool_size):\n",
    "                patch = A[i : i + self.pool_size, j : j + self.pool_size]\n",
    "                yield patch, i, j\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.last_input = A\n",
    "        A_row, A_col, num_filters = A.shape\n",
    "        output = np.zeros((A_row // self.pool_size, A_col // self.pool_size, num_filters))\n",
    "\n",
    "        for patch, i, j in self.generate_patches(A):\n",
    "            output[i//self.pool_size, j//self.pool_size] = np.amax(patch, axis=(0, 1))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_loss_d_output):\n",
    "        d_loss_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "        # Iterate over the regions as in forward pass\n",
    "        for patch, row, col in self.generate_patches(self.last_input):\n",
    "            patch_height, patch_width, num_channels = patch.shape\n",
    "            max_values = np.amax(patch, axis=(0, 1))\n",
    "\n",
    "            for i in range(patch_height):\n",
    "                for j in range(patch_width):\n",
    "                    for channel in range(num_channels):\n",
    "                        # Find the pizel that was the max value in this patch, and pass the gradient back to it\n",
    "                        if patch[i, j, channel] == max_values[channel]:\n",
    "                            d_loss_d_input[row + i, col + j, channel] = d_loss_d_output[row // self.pool_size, col // self.pool_size, channel]\n",
    "        \n",
    "        return d_loss_d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 2)\n",
      "[[-0.06205186  0.88474763  0.75923349]\n",
      " [ 0.43574185  0.3678465   0.27213616]\n",
      " [-0.10095214  0.20514779  0.8290021 ]]\n"
     ]
    }
   ],
   "source": [
    "max_pool = MaxPool2d(2)\n",
    "output = max_pool.forward(output)\n",
    "print(output.shape)\n",
    "print(output[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        self.weights = np.random.randn(num_inputs, num_outputs) / num_inputs\n",
    "        self.bias = np.zeros(num_outputs)\n",
    "    \n",
    "    def softmax(self, A):\n",
    "        expA = np.exp(A)\n",
    "        return expA / expA.sum(axis=0)\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.shape_before_flattening = A.shape\n",
    "        A = A.flatten()\n",
    "        self.input_to_last_layer = A\n",
    "        input_size, output_size = self.weights.shape\n",
    "        self.logits = np.dot(A, self.weights) + self.bias\n",
    "        return self.softmax(self.logits)\n",
    "\n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        for i, gradient in enumerate(d_L_d_out):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "\n",
    "            # e^t for every element of the output\n",
    "            a_exp = np.exp(self.logits)\n",
    "\n",
    "            # Sum of all e^t\n",
    "            S = np.sum(a_exp)\n",
    "\n",
    "            # Gradients of out[i] against totals (logits)\n",
    "            d_Z_d_a = -a_exp[i] * a_exp / (S ** 2)\n",
    "            d_Z_d_a[i] = a_exp[i] * (S - a_exp[i]) / (S ** 2)\n",
    "\n",
    "            # Gradients of totals (logits) against weights, biases, and inputs\n",
    "            d_a_d_w = self.input_to_last_layer  # Gradient of totals w.r.t weights\n",
    "            d_a_d_b = 1  # Gradient of totals w.r.t biases\n",
    "            d_a_d_inputs = self.weights  # Gradient of totals w.r.t inputs\n",
    "\n",
    "            # Gradient of the loss w.r.t totals (logits)\n",
    "            d_L_d_t = gradient * d_Z_d_a\n",
    "\n",
    "            # Gradient of the loss w.r.t weights, biases, and inputs\n",
    "            d_L_d_w = d_a_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
    "            d_L_d_b = d_L_d_t * d_a_d_b\n",
    "            d_L_d_inputs = d_a_d_inputs @ d_L_d_t\n",
    "\n",
    "            # Update weights and biases using gradient descent\n",
    "            self.weights -= learn_rate * d_L_d_w\n",
    "            self.bias -= learn_rate * d_L_d_b\n",
    "\n",
    "            # Return the gradient of the loss w.r.t inputs\n",
    "            return d_L_d_inputs.reshape(self.shape_before_flattening)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2141682  0.18942    0.19183843 0.18279042 0.22178295]\n"
     ]
    }
   ],
   "source": [
    "linear_layer = LinearLayer(18, 5)\n",
    "output = linear_layer.forward(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = pd.read_csv('mnist_train.csv')\n",
    "mnist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28)\n",
      "(1000, 10)\n",
      "(500, 28, 28)\n",
      "(500,)\n",
      "[[  1 154 253  90   0   0   0   0   0   0]\n",
      " [  0 139 253 190   2   0   0   0   0   0]\n",
      " [  0  11 190 253  70   0   0   0   0   0]\n",
      " [  0   0  35 241 225 160 108   1   0   0]\n",
      " [  0   0   0  81 240 253 253 119  25   0]\n",
      " [  0   0   0   0  45 186 253 253 150  27]\n",
      " [  0   0   0   0   0  16  93 252 253 187]\n",
      " [  0   0   0   0   0   0   0 249 253 249]\n",
      " [  0   0   0   0  46 130 183 253 253 207]\n",
      " [  0   0  39 148 229 253 253 253 250 182]]\n"
     ]
    }
   ],
   "source": [
    "data = mnist.values\n",
    "split_index = int(0.8 * data.shape[0])\n",
    "data_train = data[:split_index]\n",
    "data_test = data[split_index:]\n",
    "\n",
    "# get random sample of 1000 for training\n",
    "X = data[:1000, 1:].reshape(-1, 28, 28)\n",
    "y = data[:1000, 0]\n",
    "\n",
    "# get random sample of 500 for testing\n",
    "X_test = data[2000:2500, 1:].reshape(-1, 28, 28)\n",
    "y_test = data[2000:2500, 0]\n",
    "\n",
    "y_one_hot = np.zeros((y.shape[0], 10))\n",
    "y_one_hot[np.arange(y.shape[0]), y] = 1\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y_one_hot.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(X[0, 10:20, 10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcMElEQVR4nO3df2xV9f3H8dcttFfE9rJa2tsrPyyg4uTHFKXr1A5HR6nG8SsLKlnQqARWdMD8MZYpqEs6WeKcjun+2GBmIs5MIBpHxGJL1IIDIcRsNrSpo6Y/mMTeW4q0rP18/+DrnVda8Fzu7fu2PB/JJ+k957zvefvxpC/OPafn+pxzTgAA9LM06wYAAOcnAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmhlo38FU9PT1qampSZmamfD6fdTsAAI+cc2pvb1coFFJaWt/nOSkXQE1NTRo9erR1GwCAc9TY2KhRo0b1uT7lPoLLzMy0bgEAkABn+32etABav369Lr30Ul1wwQUqLCzU+++//7Xq+NgNAAaHs/0+T0oAvfzyy1q1apXWrFmjDz74QFOnTlVpaamOHDmSjN0BAAYilwTTp0935eXl0dfd3d0uFAq5ioqKs9aGw2EnicFgMBgDfITD4TP+vk/4GVBXV5f27dunkpKS6LK0tDSVlJSopqbmtO07OzsViURiBgBg8Et4AH366afq7u5WXl5ezPK8vDy1tLSctn1FRYUCgUB0cAccAJwfzO+CW716tcLhcHQ0NjZatwQA6AcJ/zugnJwcDRkyRK2trTHLW1tbFQwGT9ve7/fL7/cnug0AQIpL+BlQRkaGpk2bpsrKyuiynp4eVVZWqqioKNG7AwAMUEl5EsKqVau0ePFiXXvttZo+fbqefvppdXR06K677krG7gAAA1BSAmjhwoX6z3/+o0cffVQtLS361re+pe3bt592YwIA4Pzlc8456ya+LBKJKBAIWLcBADhH4XBYWVlZfa43vwsOAHB+IoAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGBiqHUDAL6ezMxMzzUXXXRRXPu65ZZbPNeMHDnSc81TTz3luaazs9NzDVITZ0AAABMEEADARMIDaO3atfL5fDFj4sSJid4NAGCAS8o1oKuuukpvvfXW/3YylEtNAIBYSUmGoUOHKhgMJuOtAQCDRFKuAR06dEihUEjjxo3TokWLdPjw4T637ezsVCQSiRkAgMEv4QFUWFiojRs3avv27XruuefU0NCgG2+8Ue3t7b1uX1FRoUAgEB2jR49OdEsAgBTkc865ZO6gra1NY8eO1VNPPaW77777tPWdnZ0x9/VHIhFCCOgFfwd0Cn8HNHCEw2FlZWX1uT7pdweMGDFCl19+uerq6npd7/f75ff7k90GACDFJP3vgI4dO6b6+nrl5+cne1cAgAEk4QH0wAMPqLq6Wh9//LHee+89zZs3T0OGDNHtt9+e6F0BAAawhH8E98knn+j222/X0aNHNXLkSN1www3avXt3XJ8PAwAGr4QH0ObNmxP9lkBKu/TSSz3XPPzww55rioqKPNdMmjTJc01/iuej+fvvvz8JncACz4IDAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgIunfiOpVJBJRIBCwbgMD3MSJE+OqW7FiheeaRYsWea4ZNmyY5xqfz+e5prGx0XONJLW3t3uuufLKKz3XfPrpp55rZsyY4bnmo48+8lyDc3e2b0TlDAgAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYGKodQM4v8TzpPMnn3zSc83ChQs910hSZmZmXHX94dChQ55rSktL49pXenq655p4njidk5PTLzVITZwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMHDSNGv5s2b57nmnnvuSUInturr6z3XfP/73/dc09jY6LlGkiZMmBBXHeAFZ0AAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBM8DBS9Ksf/vCH1i2c0ccff+y55h//+IfnmocffthzTbwPFo3HlVde2W/7wvmLMyAAgAkCCABgwnMA7dq1S7feeqtCoZB8Pp+2bt0as945p0cffVT5+fkaNmyYSkpKdOjQoUT1CwAYJDwHUEdHh6ZOnar169f3un7dunV65pln9Pzzz2vPnj0aPny4SktLdeLEiXNuFgAweHi+CaGsrExlZWW9rnPO6emnn9YvfvELzZkzR5L0wgsvKC8vT1u3btVtt912bt0CAAaNhF4DamhoUEtLi0pKSqLLAoGACgsLVVNT02tNZ2enIpFIzAAADH4JDaCWlhZJUl5eXszyvLy86LqvqqioUCAQiI7Ro0cnsiUAQIoyvwtu9erVCofD0dGff+sAALCT0AAKBoOSpNbW1pjlra2t0XVf5ff7lZWVFTMAAINfQgOooKBAwWBQlZWV0WWRSER79uxRUVFRIncFABjgPN8Fd+zYMdXV1UVfNzQ06MCBA8rOztaYMWO0YsUK/fKXv9Rll12mgoICPfLIIwqFQpo7d24i+wYADHCeA2jv3r266aaboq9XrVolSVq8eLE2btyohx56SB0dHVqyZIna2tp0ww03aPv27brgggsS1zUAYMDzOeecdRNfFolEFAgErNtAkoRCIc81S5Ys8Vzz5ptveq6RFHN2/3UdOXIkrn2lsnvuucdzzfPPP5+ETk43Y8YMzzXvvPNO4hvBWYXD4TNe1ze/Cw4AcH4igAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJjw/HUMwLloamryXLN27drEN4Iz4gsk0R84AwIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCh5EC5+j+++/3XDN8+PAkdJI4kydP7pf9vPfee55rampqktAJLHAGBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQPI0XKu/DCCz3XfPOb34xrX2vWrPFcc/PNN8e1L6/S0rz/e7GnpycJnfSuqanJc81dd93luaa7u9tzDVITZ0AAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBM8DBSxC09Pd1zzdVXX+255m9/+5vnmvz8fM81kvT55597ronnIZw1NTWea2bPnu25Jp4HucZr6FDvv07mz5/vuea3v/2t55quri7PNUg+zoAAACYIIACACc8BtGvXLt16660KhULy+XzaunVrzPo777xTPp8vZsTz0QEAYHDzHEAdHR2aOnWq1q9f3+c2s2fPVnNzc3S89NJL59QkAGDw8XzVsKysTGVlZWfcxu/3KxgMxt0UAGDwS8o1oKqqKuXm5uqKK67QsmXLdPTo0T637ezsVCQSiRkAgMEv4QE0e/ZsvfDCC6qsrNSTTz6p6upqlZWV9fk97hUVFQoEAtExevToRLcEAEhBCf87oNtuuy368+TJkzVlyhSNHz9eVVVVmjlz5mnbr169WqtWrYq+jkQihBAAnAeSfhv2uHHjlJOTo7q6ul7X+/1+ZWVlxQwAwOCX9AD65JNPdPTo0bj/Mh0AMDh5/gju2LFjMWczDQ0NOnDggLKzs5Wdna3HHntMCxYsUDAYVH19vR566CFNmDBBpaWlCW0cADCweQ6gvXv36qabboq+/uL6zeLFi/Xcc8/p4MGD+vOf/6y2tjaFQiHNmjVLTzzxhPx+f+K6BgAMeD7nnLNu4ssikYgCgYB1G+eVjIyMuOriecLFq6++Gte+vHrsscfiqtu5c6fnmnfffddzTXZ2tueaeHqbNGmS55pUt2jRIs81X31iy9fV2dkZVx1OCYfDZ7yuz7PgAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmeBr2IJOenu655vHHH49rXw8++GBcdV79/e9/91zzox/9KK59tbW1ea4ZOXKk55o33njDc80111zjuaarq8tzjSStW7fOc008T96eM2eO55p4vPXWW3HVPfnkk55rPvvss7j25dWBAwf6ZT/ngqdhAwBSEgEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABNDrRtA34YMGeK55oknnvBc88ADD3iukaSOjg7PNT/72c8812zevNlzTTwPFZWka6+91nPN7373O881V199teeaQ4cOea5ZtmyZ5xpJevvttz3XnOmhk335zne+47lm0aJFnmt+8IMfeK6RpB07dsRV51VjY6PnmoKCgiR00r84AwIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGDC55xz1k18WSQSUSAQsG4jJcTzIMlnn33Wc83x48c910jSkiVLPNe8+eabnmsKCws919x1112eaySprKzMc82wYcM81zz++OOeazZs2OC5Jp6HXA5Gt99+e1x1d9xxR4I76d3KlSs919TV1SWhk8QKh8NnfEgtZ0AAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBM8DDSFNbc3Oy5ZuTIkZ5rOjs7PddI0kcffeS5Zvjw4Z5rJkyY4LmmP61du9ZzTUVFheea7u5uzzWAJR5GCgBISQQQAMCEpwCqqKjQddddp8zMTOXm5mru3Lmqra2N2ebEiRMqLy/XxRdfrIsuukgLFixQa2trQpsGAAx8ngKourpa5eXl2r17t3bs2KGTJ09q1qxZ6ujoiG6zcuVKvfbaa3rllVdUXV2tpqYmzZ8/P+GNAwAGtqFeNt6+fXvM640bNyo3N1f79u1TcXGxwuGw/vjHP2rTpk363ve+J+nUtzheeeWV2r17t7797W8nrnMAwIB2TteAwuGwJCk7O1uStG/fPp08eVIlJSXRbSZOnKgxY8aopqam1/fo7OxUJBKJGQCAwS/uAOrp6dGKFSt0/fXXa9KkSZKklpYWZWRkaMSIETHb5uXlqaWlpdf3qaioUCAQiI7Ro0fH2xIAYACJO4DKy8v14YcfavPmzefUwOrVqxUOh6OjsbHxnN4PADAweLoG9IXly5fr9ddf165duzRq1Kjo8mAwqK6uLrW1tcWcBbW2tioYDPb6Xn6/X36/P542AAADmKczIOecli9fri1btmjnzp0qKCiIWT9t2jSlp6ersrIyuqy2tlaHDx9WUVFRYjoGAAwKns6AysvLtWnTJm3btk2ZmZnR6zqBQEDDhg1TIBDQ3XffrVWrVik7O1tZWVm67777VFRUxB1wAIAYngLoueeekyTNmDEjZvmGDRt05513SpJ+85vfKC0tTQsWLFBnZ6dKS0v1+9//PiHNAgAGDx5GmsL279/vuWby5MlJ6MTWG2+84blm165dce1r69atnms+/vhjzzX//e9/PdcAAw0PIwUApCQCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgIm4vhEV/aO4uNhzzdy5cz3XXHPNNZ5rJOnIkSOea/70pz95rvnss88813R1dXmuAdC/OAMCAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgwuecc9ZNfFkkElEgELBuAwBwjsLhsLKysvpczxkQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABOeAqiiokLXXXedMjMzlZubq7lz56q2tjZmmxkzZsjn88WMpUuXJrRpAMDA5ymAqqurVV5ert27d2vHjh06efKkZs2apY6Ojpjt7r33XjU3N0fHunXrEto0AGDgG+pl4+3bt8e83rhxo3Jzc7Vv3z4VFxdHl1944YUKBoOJ6RAAMCid0zWgcDgsScrOzo5Z/uKLLyonJ0eTJk3S6tWrdfz48T7fo7OzU5FIJGYAAM4DLk7d3d3ulltucddff33M8j/84Q9u+/bt7uDBg+4vf/mLu+SSS9y8efP6fJ81a9Y4SQwGg8EYZCMcDp8xR+IOoKVLl7qxY8e6xsbGM25XWVnpJLm6urpe1584ccKFw+HoaGxsNJ80BoPBYJz7OFsAeboG9IXly5fr9ddf165duzRq1KgzbltYWChJqqur0/jx409b7/f75ff742kDADCAeQog55zuu+8+bdmyRVVVVSooKDhrzYEDByRJ+fn5cTUIABicPAVQeXm5Nm3apG3btikzM1MtLS2SpEAgoGHDhqm+vl6bNm3SzTffrIsvvlgHDx7UypUrVVxcrClTpiTlPwAAMEB5ue6jPj7n27Bhg3POucOHD7vi4mKXnZ3t/H6/mzBhgnvwwQfP+jngl4XDYfPPLRkMBoNx7uNsv/t9/x8sKSMSiSgQCFi3AQA4R+FwWFlZWX2u51lwAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATKRdAzjnrFgAACXC23+cpF0Dt7e3WLQAAEuBsv899LsVOOXp6etTU1KTMzEz5fL6YdZFIRKNHj1ZjY6OysrKMOrTHPJzCPJzCPJzCPJySCvPgnFN7e7tCoZDS0vo+zxnajz19LWlpaRo1atQZt8nKyjqvD7AvMA+nMA+nMA+nMA+nWM9DIBA46zYp9xEcAOD8QAABAEwMqADy+/1as2aN/H6/dSummIdTmIdTmIdTmIdTBtI8pNxNCACA88OAOgMCAAweBBAAwAQBBAAwQQABAEwMmABav369Lr30Ul1wwQUqLCzU+++/b91Sv1u7dq18Pl/MmDhxonVbSbdr1y7deuutCoVC8vl82rp1a8x655weffRR5efna9iwYSopKdGhQ4dsmk2is83DnXfeedrxMXv2bJtmk6SiokLXXXedMjMzlZubq7lz56q2tjZmmxMnTqi8vFwXX3yxLrroIi1YsECtra1GHSfH15mHGTNmnHY8LF261Kjj3g2IAHr55Ze1atUqrVmzRh988IGmTp2q0tJSHTlyxLq1fnfVVVepubk5Ot555x3rlpKuo6NDU6dO1fr163tdv27dOj3zzDN6/vnntWfPHg0fPlylpaU6ceJEP3eaXGebB0maPXt2zPHx0ksv9WOHyVddXa3y8nLt3r1bO3bs0MmTJzVr1ix1dHREt1m5cqVee+01vfLKK6qurlZTU5Pmz59v2HXifZ15kKR777035nhYt26dUcd9cAPA9OnTXXl5efR1d3e3C4VCrqKiwrCr/rdmzRo3depU6zZMSXJbtmyJvu7p6XHBYND9+te/ji5ra2tzfr/fvfTSSwYd9o+vzoNzzi1evNjNmTPHpB8rR44ccZJcdXW1c+7U//v09HT3yiuvRLf517/+5SS5mpoaqzaT7qvz4Jxz3/3ud91PfvITu6a+hpQ/A+rq6tK+fftUUlISXZaWlqaSkhLV1NQYdmbj0KFDCoVCGjdunBYtWqTDhw9bt2SqoaFBLS0tMcdHIBBQYWHheXl8VFVVKTc3V1dccYWWLVumo0ePWreUVOFwWJKUnZ0tSdq3b59OnjwZczxMnDhRY8aMGdTHw1fn4QsvvviicnJyNGnSJK1evVrHjx+3aK9PKfcw0q/69NNP1d3drby8vJjleXl5+uijj4y6slFYWKiNGzfqiiuuUHNzsx577DHdeOON+vDDD5WZmWndnomWlhZJ6vX4+GLd+WL27NmaP3++CgoKVF9fr5///OcqKytTTU2NhgwZYt1ewvX09GjFihW6/vrrNWnSJEmnjoeMjAyNGDEiZtvBfDz0Ng+SdMcdd2js2LEKhUI6ePCgHn74YdXW1urVV1817DZWygcQ/qesrCz685QpU1RYWKixY8fqr3/9q+6++27DzpAKbrvttujPkydP1pQpUzR+/HhVVVVp5syZhp0lR3l5uT788MPz4jromfQ1D0uWLIn+PHnyZOXn52vmzJmqr6/X+PHj+7vNXqX8R3A5OTkaMmTIaXextLa2KhgMGnWVGkaMGKHLL79cdXV11q2Y+eIY4Pg43bhx45STkzMoj4/ly5fr9ddf19tvvx3z9S3BYFBdXV1qa2uL2X6wHg99zUNvCgsLJSmljoeUD6CMjAxNmzZNlZWV0WU9PT2qrKxUUVGRYWf2jh07pvr6euXn51u3YqagoEDBYDDm+IhEItqzZ895f3x88sknOnr06KA6PpxzWr58ubZs2aKdO3eqoKAgZv20adOUnp4eczzU1tbq8OHDg+p4ONs89ObAgQOSlFrHg/VdEF/H5s2bnd/vdxs3bnT//Oc/3ZIlS9yIESNcS0uLdWv96qc//amrqqpyDQ0N7t1333UlJSUuJyfHHTlyxLq1pGpvb3f79+93+/fvd5LcU0895fbv3+/+/e9/O+ec+9WvfuVGjBjhtm3b5g4ePOjmzJnjCgoK3Oeff27ceWKdaR7a29vdAw884GpqalxDQ4N766233DXXXOMuu+wyd+LECevWE2bZsmUuEAi4qqoq19zcHB3Hjx+PbrN06VI3ZswYt3PnTrd3715XVFTkioqKDLtOvLPNQ11dnXv88cfd3r17XUNDg9u2bZsbN26cKy4uNu481oAIIOece/bZZ92YMWNcRkaGmz59utu9e7d1S/1u4cKFLj8/32VkZLhLLrnELVy40NXV1Vm3lXRvv/22k3TaWLx4sXPu1K3YjzzyiMvLy3N+v9/NnDnT1dbW2jadBGeah+PHj7tZs2a5kSNHuvT0dDd27Fh37733Drp/pPX23y/JbdiwIbrN559/7n784x+7b3zjG+7CCy908+bNc83NzXZNJ8HZ5uHw4cOuuLjYZWdnO7/f7yZMmOAefPBBFw6HbRv/Cr6OAQBgIuWvAQEABicCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAm/g9Ise1Z6nwvCQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_img(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "show_img(X[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, num_classes):\n",
    "        self.conv = ConvolutionLayer(3, 8)\n",
    "        self.pool = MaxPool2d(2)\n",
    "        # self.linear = LinearLayer(13 * 13 * 8, num_classes)\n",
    "        self.linear = LinearLayer(14 * 14 * 8, num_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, A):\n",
    "        output = self.conv.forward(A)\n",
    "        # print(\"After Convolution Layer:\")\n",
    "        # print(output.shape)\n",
    "        # sep()\n",
    "\n",
    "        output = self.pool.forward(output)\n",
    "        # print(\"After Max Pooling Layer:\")\n",
    "        # print(output.shape)\n",
    "        # sep()\n",
    "        \n",
    "        output = self.linear.forward(output)\n",
    "        # print(\"After Linear Layer:\")\n",
    "        # print(output.shape)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backprop(self, y, y_hat, learn_rate):\n",
    "        grad = np.zeros(y.shape)\n",
    "        grad[np.argmax(y)] = -1 / y_hat[np.argmax(y)]\n",
    "\n",
    "        grad = self.linear.backprop(grad, learn_rate)\n",
    "        grad = self.pool.backprop(grad)\n",
    "        grad = self.conv.backprop(grad, learn_rate)\n",
    "\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is it 13x13x8?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input was an image of size 28x28\n",
    "Now, we have a kernel size of 3 x 3, so we will end up with a 28 - 3 + 1 = 26 x 26 image.\n",
    "Since we have 8 filters, we will end up with 26 x 26 x 8 image after the first convolutional layer.\n",
    "\n",
    "Now, we have a max pooling layer with a pool size of 2 x 2, so each block of 2 x 2 pixels will be reduced to 1 pixel. So, we will end up with a 13 x 13 x 8 image after the first max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inp):\n",
    "    # Normalize the input to be between 0 and 1\n",
    "    inp = inp.astype(np.float32)\n",
    "    inp /= 255.0\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbwklEQVR4nO3df2xV9f3H8del0Atqe7HU9vbKrwJKF5EaEbqK4g86Sl2cKHHozILTQNBihswf6/yBbi7dWDKNG0MzDZ0Z+CsRmMw0YrUlmwUD2hCy2dCuriXQMrv03lJsYfTz/YN4v15pwXO5t++2PB/JJ+k957x73nw49uW59/CpzznnBADAABth3QAA4NxEAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMDESOsGvq63t1cHDx5UWlqafD6fdTsAAI+cc+rs7FQoFNKIEf3f5wy6ADp48KAmTJhg3QYA4Cy1tLRo/Pjx/e4fdG/BpaWlWbcAAEiAM/08T1oArVu3TpMnT9bo0aNVUFCgjz766BvV8bYbAAwPZ/p5npQAev3117V69WqtWbNGH3/8sfLz81VcXKzDhw8n43QAgKHIJcGcOXNcaWlp9PWJEydcKBRy5eXlZ6wNh8NOEoPBYDCG+AiHw6f9eZ/wO6Bjx45pz549Kioqim4bMWKEioqKVFtbe8rxPT09ikQiMQMAMPwlPIA+//xznThxQtnZ2THbs7Oz1draesrx5eXlCgQC0cETcABwbjB/Cq6srEzhcDg6WlparFsCAAyAhP87oMzMTKWkpKitrS1me1tbm4LB4CnH+/1++f3+RLcBABjkEn4HlJqaqlmzZqmqqiq6rbe3V1VVVSosLEz06QAAQ1RSVkJYvXq1li5dqquuukpz5szRc889p66uLv3oRz9KxukAAENQUgJoyZIl+s9//qMnn3xSra2tuuKKK1RZWXnKgwkAgHOXzznnrJv4qkgkokAgYN0GAOAshcNhpaen97vf/Ck4AMC5iQACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJkdYNoH+hUMhzzUUXXeS55v777/dcI0nLly/3XOOci+tckP7yl794rnnqqafiOlddXV1cdYAX3AEBAEwQQAAAEwkPoKeeeko+ny9m5OXlJfo0AIAhLimfAV122WV67733/v8kI/moCQAQKynJMHLkSAWDwWR8awDAMJGUz4D279+vUCikKVOm6K677lJzc3O/x/b09CgSicQMAMDwl/AAKigoUEVFhSorK7V+/Xo1NTXp2muvVWdnZ5/Hl5eXKxAIRMeECRMS3RIAYBBKeACVlJTo9ttv18yZM1VcXKx33nlHHR0deuONN/o8vqysTOFwODpaWloS3RIAYBBK+tMBY8eO1aWXXqqGhoY+9/v9fvn9/mS3AQAYZJL+74COHDmixsZG5eTkJPtUAIAhJOEB9NBDD6mmpkafffaZPvzwQ916661KSUnRnXfemehTAQCGsIS/BXfgwAHdeeedam9v10UXXaRrrrlGO3fujGuNMgDA8OVzg2x1yEgkokAgYN3GoNDY2Oi5ZvLkyYlvxFhbW5vnmnfffTcJnfRt6tSpnmuuvvrqJHRyqvb29rjqlixZ4rnmgw8+iOtcGL7C4bDS09P73c9acAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwk/RfSDUfx/AK93//+955rBmph0X379sVVV1FR4blm48aNnmsikYjnmu7ubs818Ro50vt/RqmpqZ5rpk2b5rmmtrbWc40U32Ku3//+9z3XbN682XMNhg/ugAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJlgNOw55eXmea+655x7PNV1dXZ5rVq5c6bnmtdde81wjSceOHYurbrj53//+NyA1e/fu9VwT79/R6NGjPddcccUVnmtYDfvcxh0QAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEyxGOoi99957nmteeeWVJHSCoWrfvn1x1V199dUJ7gQ4FXdAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATLAYaRw+++wzzzWPPvqo55o5c+Z4rhk/frznmgMHDniuwdAwY8YM6xaAfnEHBAAwQQABAEx4DqAdO3bo5ptvVigUks/n05YtW2L2O+f05JNPKicnR2PGjFFRUZH279+fqH4BAMOE5wDq6upSfn6+1q1b1+f+tWvX6vnnn9cLL7ygXbt26fzzz1dxcbG6u7vPulkAwPDh+SGEkpISlZSU9LnPOafnnntOjz/+uG655RZJJ39DZ3Z2trZs2aI77rjj7LoFAAwbCf0MqKmpSa2trSoqKopuCwQCKigoUG1tbZ81PT09ikQiMQMAMPwlNIBaW1slSdnZ2THbs7Ozo/u+rry8XIFAIDomTJiQyJYAAIOU+VNwZWVlCofD0dHS0mLdEgBgACQ0gILBoCSpra0tZntbW1t039f5/X6lp6fHDADA8JfQAMrNzVUwGFRVVVV0WyQS0a5du1RYWJjIUwEAhjjPT8EdOXJEDQ0N0ddNTU2qq6tTRkaGJk6cqFWrVumZZ57RJZdcotzcXD3xxBMKhUJatGhRIvsGAAxxngNo9+7duuGGG6KvV69eLUlaunSpKioq9Mgjj6irq0vLly9XR0eHrrnmGlVWVmr06NGJ6xoAMOT5nHPOuomvikQiCgQC1m0Ag84VV1zhuebDDz+M61x+v99zzTPPPOO5Zs2aNZ5rMHSEw+HTfq5v/hQcAODcRAABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAw4fnXMQA4e1dddZXnmm3btnmuiWdVa0lqb2/3XPPOO+/EdS6cu7gDAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYILFSDEspaSkDFhdXl6e55p4Fu4cN26c55p4vfTSS55rdu/enYROMJxxBwQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMCEzznnrJv4qkgkokAgYN0GBpFrrrnGc83atWvjOldBQUFcdZDq6uo81zz22GOeayorKz3XwEY4HFZ6enq/+7kDAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYILFSDGg0tLSPNc0NTV5rrnwwgs91wx2//rXvzzXTJgwIa5zjRo1Kq46rzo7Oz3X3H777Z5rtm/f7rkGZ4/FSAEAgxIBBAAw4TmAduzYoZtvvlmhUEg+n09btmyJ2X/33XfL5/PFjIULFyaqXwDAMOE5gLq6upSfn69169b1e8zChQt16NCh6Hj11VfPqkkAwPAz0mtBSUmJSkpKTnuM3+9XMBiMuykAwPCXlM+AqqurlZWVpenTp+u+++5Te3t7v8f29PQoEonEDADA8JfwAFq4cKFeeeUVVVVV6de//rVqampUUlKiEydO9Hl8eXm5AoFAdMT72CgAYGjx/Bbcmdxxxx3Rry+//HLNnDlTU6dOVXV1tebPn3/K8WVlZVq9enX0dSQSIYQA4ByQ9Mewp0yZoszMTDU0NPS53+/3Kz09PWYAAIa/pAfQgQMH1N7erpycnGSfCgAwhHh+C+7IkSMxdzNNTU2qq6tTRkaGMjIy9PTTT2vx4sUKBoNqbGzUI488omnTpqm4uDihjQMAhjbPAbR7927dcMMN0ddffn6zdOlSrV+/Xnv37tWf/vQndXR0KBQKacGCBfrFL34hv9+fuK4BAEMei5FiQPl8Ps8199xzj+eaVatWea6J11tvveW55pe//KXnmv6eJD2deOZbkiZPnuy55tlnn/Vcc9NNN3muOXr0qOeaeB9s6ujoiKsOJ7EYKQBgUCKAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmGA1bAAJMX36dM817777ruea8ePHe66JZ8VySbr99tvjqsNJrIYNABiUCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmBhp3QCA4aG+vt5zzUcffeS5Jp7FSIPBoOcaJB93QAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEywGCmAhMjPz/dcM3fu3CR0gqGCOyAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmWIwUA+rCCy/0XLNlyxbPNZWVlZ5rJKm8vDyuusEqJSUlrrof/vCHnmvimbusrCzPNcePH/dc8+ijj3quQfJxBwQAMEEAAQBMeAqg8vJyzZ49W2lpacrKytKiRYtUX18fc0x3d7dKS0s1btw4XXDBBVq8eLHa2toS2jQAYOjzFEA1NTUqLS3Vzp07tX37dh0/flwLFixQV1dX9JgHH3xQb7/9tt58803V1NTo4MGDuu222xLeOABgaPP0EMLXP9itqKhQVlaW9uzZo3nz5ikcDuvll1/Wpk2bdOONN0qSNmzYoG9961vauXOnvv3tbyeucwDAkHZWnwGFw2FJUkZGhiRpz549On78uIqKiqLH5OXlaeLEiaqtre3ze/T09CgSicQMAMDwF3cA9fb2atWqVZo7d65mzJghSWptbVVqaqrGjh0bc2x2drZaW1v7/D7l5eUKBALRMWHChHhbAgAMIXEHUGlpqfbt26fXXnvtrBooKytTOByOjpaWlrP6fgCAoSGuf4i6cuVKbdu2TTt27ND48eOj24PBoI4dO6aOjo6Yu6C2tjYFg8E+v5ff75ff74+nDQDAEObpDsg5p5UrV2rz5s16//33lZubG7N/1qxZGjVqlKqqqqLb6uvr1dzcrMLCwsR0DAAYFjzdAZWWlmrTpk3aunWr0tLSop/rBAIBjRkzRoFAQPfee69Wr16tjIwMpaen64EHHlBhYSFPwAEAYngKoPXr10uSrr/++pjtGzZs0N133y1JevbZZzVixAgtXrxYPT09Ki4u1h/+8IeENAsAGD58zjln3cRXRSIRBQIB6zbwDVx55ZWeayoqKjzXxLOA6ezZsz3XSOr3ac3B4KqrrvJc89hjj8V1ru9973tx1Xn13//+13PNyy+/7Lnmpz/9qecanL1wOKz09PR+97MWHADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARFy/ERWQpOzsbM81l112WRI6OdUf//jHuOqqq6s91+Tl5Xmuue666zzXXHzxxZ5rRo8e7bkmXp2dnZ5rHn/8cc81L774oucaDE7cAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADDhc8456ya+KhKJKBAIWLeBbyCexTErKio819x4442ea3BSc3NzXHUvvfSS55q//vWvnmvq6uo812DoCIfDSk9P73c/d0AAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMsBgpBlRKSornmu985zueaxYtWuS5RpKWLVvmuebTTz/1XFNdXe25pqGhwXPNxo0bPddI0uHDh+OqA76KxUgBAIMSAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEyxGCgBIChYjBQAMSgQQAMCEpwAqLy/X7NmzlZaWpqysLC1atEj19fUxx1x//fXy+XwxY8WKFQltGgAw9HkKoJqaGpWWlmrnzp3avn27jh8/rgULFqirqyvmuGXLlunQoUPRsXbt2oQ2DQAY+kZ6ObiysjLmdUVFhbKysrRnzx7Nmzcvuv28885TMBhMTIcAgGHprD4DCofDkqSMjIyY7Rs3blRmZqZmzJihsrIyHT16tN/v0dPTo0gkEjMAAOcAF6cTJ0647373u27u3Lkx21988UVXWVnp9u7d6/785z+7iy++2N166639fp81a9Y4SQwGg8EYZiMcDp82R+IOoBUrVrhJkya5lpaW0x5XVVXlJLmGhoY+93d3d7twOBwdLS0t5pPGYDAYjLMfZwogT58BfWnlypXatm2bduzYofHjx5/22IKCAklSQ0ODpk6desp+v98vv98fTxsAgCHMUwA55/TAAw9o8+bNqq6uVm5u7hlr6urqJEk5OTlxNQgAGJ48BVBpaak2bdqkrVu3Ki0tTa2trZKkQCCgMWPGqLGxUZs2bdJNN92kcePGae/evXrwwQc1b948zZw5Myl/AADAEOXlcx/18z7fhg0bnHPONTc3u3nz5rmMjAzn9/vdtGnT3MMPP3zG9wG/KhwOm79vyWAwGIyzH2f62c9ipACApGAxUgDAoEQAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMDHoAsg5Z90CACABzvTzfNAFUGdnp3ULAIAEONPPc58bZLccvb29OnjwoNLS0uTz+WL2RSIRTZgwQS0tLUpPTzfq0B7zcBLzcBLzcBLzcNJgmAfnnDo7OxUKhTRiRP/3OSMHsKdvZMSIERo/fvxpj0lPTz+nL7AvMQ8nMQ8nMQ8nMQ8nWc9DIBA44zGD7i04AMC5gQACAJgYUgHk9/u1Zs0a+f1+61ZMMQ8nMQ8nMQ8nMQ8nDaV5GHQPIQAAzg1D6g4IADB8EEAAABMEEADABAEEADAxZAJo3bp1mjx5skaPHq2CggJ99NFH1i0NuKeeeko+ny9m5OXlWbeVdDt27NDNN9+sUCgkn8+nLVu2xOx3zunJJ59UTk6OxowZo6KiIu3fv9+m2SQ60zzcfffdp1wfCxcutGk2ScrLyzV79mylpaUpKytLixYtUn19fcwx3d3dKi0t1bhx43TBBRdo8eLFamtrM+o4Ob7JPFx//fWnXA8rVqww6rhvQyKAXn/9da1evVpr1qzRxx9/rPz8fBUXF+vw4cPWrQ24yy67TIcOHYqOv/3tb9YtJV1XV5fy8/O1bt26PvevXbtWzz//vF544QXt2rVL559/voqLi9Xd3T3AnSbXmeZBkhYuXBhzfbz66qsD2GHy1dTUqLS0VDt37tT27dt1/PhxLViwQF1dXdFjHnzwQb399tt68803VVNTo4MHD+q2224z7Drxvsk8SNKyZctiroe1a9caddwPNwTMmTPHlZaWRl+fOHHChUIhV15ebtjVwFuzZo3Lz8+3bsOUJLd58+bo697eXhcMBt1vfvOb6LaOjg7n9/vdq6++atDhwPj6PDjn3NKlS90tt9xi0o+Vw4cPO0mupqbGOXfy737UqFHuzTffjB7zz3/+00lytbW1Vm0m3dfnwTnnrrvuOvfjH//YrqlvYNDfAR07dkx79uxRUVFRdNuIESNUVFSk2tpaw85s7N+/X6FQSFOmTNFdd92l5uZm65ZMNTU1qbW1Neb6CAQCKigoOCevj+rqamVlZWn69Om677771N7ebt1SUoXDYUlSRkaGJGnPnj06fvx4zPWQl5eniRMnDuvr4evz8KWNGzcqMzNTM2bMUFlZmY4ePWrRXr8G3WKkX/f555/rxIkTys7OjtmenZ2tTz/91KgrGwUFBaqoqND06dN16NAhPf3007r22mu1b98+paWlWbdnorW1VZL6vD6+3HeuWLhwoW677Tbl5uaqsbFRP/vZz1RSUqLa2lqlpKRYt5dwvb29WrVqlebOnasZM2ZIOnk9pKamauzYsTHHDufroa95kKQf/OAHmjRpkkKhkPbu3atHH31U9fX1euuttwy7jTXoAwj/r6SkJPr1zJkzVVBQoEmTJumNN97Qvffea9gZBoM77rgj+vXll1+umTNnaurUqaqurtb8+fMNO0uO0tJS7du375z4HPR0+puH5cuXR7++/PLLlZOTo/nz56uxsVFTp04d6Db7NOjfgsvMzFRKSsopT7G0tbUpGAwadTU4jB07VpdeeqkaGhqsWzHz5TXA9XGqKVOmKDMzc1heHytXrtS2bdv0wQcfxPz6lmAwqGPHjqmjoyPm+OF6PfQ3D30pKCiQpEF1PQz6AEpNTdWsWbNUVVUV3dbb26uqqioVFhYadmbvyJEjamxsVE5OjnUrZnJzcxUMBmOuj0gkol27dp3z18eBAwfU3t4+rK4P55xWrlypzZs36/3331dubm7M/lmzZmnUqFEx10N9fb2am5uH1fVwpnnoS11dnSQNruvB+imIb+K1115zfr/fVVRUuH/84x9u+fLlbuzYsa61tdW6tQH1k5/8xFVXV7umpib397//3RUVFbnMzEx3+PBh69aSqrOz033yySfuk08+cZLcb3/7W/fJJ5+4f//738455371q1+5sWPHuq1bt7q9e/e6W265xeXm5rovvvjCuPPEOt08dHZ2uoceesjV1ta6pqYm995777krr7zSXXLJJa67u9u69YS57777XCAQcNXV1e7QoUPRcfTo0egxK1ascBMnTnTvv/++2717tyssLHSFhYWGXSfemeahoaHB/fznP3e7d+92TU1NbuvWrW7KlClu3rx5xp3HGhIB5Jxzv/vd79zEiRNdamqqmzNnjtu5c6d1SwNuyZIlLicnx6WmprqLL77YLVmyxDU0NFi3lXQffPCBk3TKWLp0qXPu5KPYTzzxhMvOznZ+v9/Nnz/f1dfX2zadBKebh6NHj7oFCxa4iy66yI0aNcpNmjTJLVu2bNj9T1pff35JbsOGDdFjvvjiC3f//fe7Cy+80J133nnu1ltvdYcOHbJrOgnONA/Nzc1u3rx5LiMjw/n9fjdt2jT38MMPu3A4bNv41/DrGAAAJgb9Z0AAgOGJAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACAif8DbkHklNdIMRYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_index = np.random.randint(0, X.shape[0])\n",
    "inp = pad(X[random_index], 1)\n",
    "inp = normalize(inp)\n",
    "\n",
    "output = cnn.forward(inp)\n",
    "print(output.shape)\n",
    "print(np.argmax(output))\n",
    "show_img(X[random_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, y_hat):\n",
    "    return -np.sum(y * np.log(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_accuracy():\n",
    "    correct = 0\n",
    "    for i in range(X_test.shape[0]):\n",
    "        inp = pad(X_test[i], 1)\n",
    "        inp = normalize(inp)\n",
    "        output = cnn.forward(inp)\n",
    "        if np.argmax(output) == y_test[i]:\n",
    "            correct += 1\n",
    "    return correct / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(im, label, lr=.001):\n",
    "    # Forward\n",
    "    out = cnn.forward(im)\n",
    "    loss = cross_entropy_loss(label, out)\n",
    "    cnn.backprop(label, out, lr)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.132071279743518, Accuracy: 0.325\n",
      "Epoch 2, Loss: 1.3201957635926433, Accuracy: 0.389\n",
      "Epoch 3, Loss: 0.7081698657894987, Accuracy: 0.408\n",
      "Epoch 4, Loss: 0.5292081700144442, Accuracy: 0.421\n",
      "Epoch 5, Loss: 0.45171967755437237, Accuracy: 0.426\n",
      "Epoch 6, Loss: 0.4053385787444739, Accuracy: 0.429\n",
      "Epoch 7, Loss: 0.37256030805032414, Accuracy: 0.432\n",
      "Epoch 8, Loss: 0.3470806866577148, Accuracy: 0.432\n",
      "Epoch 9, Loss: 0.3260483590991805, Accuracy: 0.432\n",
      "Epoch 10, Loss: 0.3080034161884252, Accuracy: 0.431\n",
      "Epoch 11, Loss: 0.29211971303313367, Accuracy: 0.429\n",
      "Epoch 12, Loss: 0.27789439908990143, Accuracy: 0.429\n",
      "Epoch 13, Loss: 0.2649950165968772, Accuracy: 0.427\n",
      "Epoch 14, Loss: 0.2531902595272727, Accuracy: 0.427\n",
      "Epoch 15, Loss: 0.24230658796905535, Accuracy: 0.429\n",
      "Epoch 16, Loss: 0.23220840149593075, Accuracy: 0.428\n",
      "Epoch 17, Loss: 0.22278413432419591, Accuracy: 0.429\n",
      "Epoch 18, Loss: 0.21394520201414394, Accuracy: 0.428\n",
      "Epoch 19, Loss: 0.20561722974283803, Accuracy: 0.431\n",
      "Epoch 20, Loss: 0.19773991478101344, Accuracy: 0.432\n",
      "Epoch 21, Loss: 0.19026374241282362, Accuracy: 0.433\n",
      "Epoch 22, Loss: 0.18315101376791607, Accuracy: 0.433\n",
      "Epoch 23, Loss: 0.17636761534408982, Accuracy: 0.431\n",
      "Epoch 24, Loss: 0.16988377835199317, Accuracy: 0.431\n",
      "Epoch 25, Loss: 0.16367628182173963, Accuracy: 0.433\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     inp \u001b[38;5;241m=\u001b[39m pad(X[i], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m     inp \u001b[38;5;241m=\u001b[39m normalize(inp)\n\u001b[0;32m----> 6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_one_hot\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtesting_accuracy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[134], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(im, label, lr)\u001b[0m\n\u001b[1;32m      3\u001b[0m out \u001b[38;5;241m=\u001b[39m cnn\u001b[38;5;241m.\u001b[39mforward(im)\n\u001b[1;32m      4\u001b[0m loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(label, out)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[122], line 32\u001b[0m, in \u001b[0;36mCNN.backprop\u001b[0;34m(self, y, y_hat, learn_rate)\u001b[0m\n\u001b[1;32m     30\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear\u001b[38;5;241m.\u001b[39mbackprop(grad, learn_rate)\n\u001b[1;32m     31\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mbackprop(grad)\n\u001b[0;32m---> 32\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "Cell \u001b[0;32mIn[113], line 31\u001b[0m, in \u001b[0;36mConvolutionLayer.backprop\u001b[0;34m(self, d_loss_d_output, learning_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m patch, row, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_patches(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_input):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Update the gradient of the filters\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filter_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_filters):\n\u001b[0;32m---> 31\u001b[0m         d_loss_d_filters[filter_index] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss_d_output[row, col, filter_index] \u001b[38;5;241m*\u001b[39m patch\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Update filters using the calculated gradient and learning rate\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m d_loss_d_filters\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        inp = pad(X[i], 1)\n",
    "        inp = normalize(inp)\n",
    "        loss = train(inp, y_one_hot[i])\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / X.shape[0]}, Accuracy: {testing_accuracy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
